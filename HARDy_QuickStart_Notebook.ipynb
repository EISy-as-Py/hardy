{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project HARDy: Quickstart Guide\n",
    "Updated 2020-06-09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRO:\n",
    "Hail and Well-Met! \n",
    " * If you've just downloaded Project HARDy, this will be your guide to the initial setup you need in order to run our package and test it out on your data! \n",
    " * Or, if you're ready to go, Simply click \"Kernel\" > \"Restart & Run All\" in the toolbar and watch it go!\n",
    "\n",
    " * ** **Note: As of Recent Testing, the default configuration may take >4 Hours to run on a normal Personal laptop. We will discuss speed and future work at the end. For your first start, We suggest starting with a subset of your data and more simple Configuration Files!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this Quick-Start notebook, there are only **~4 Things** we need to talk about and you'll be set to go!\n",
    " 1. Your Raw Data, in CSV form\n",
    " 2. A Transformation Configuation File (That works with your data!)\n",
    " 3. The Tuner and/or Model Configuration Files\n",
    " 4. Understanding your Computer's Speed limits\n",
    "\n",
    "For $1 \\Rightarrow 3$, Let's start out by giving a Path for each one, and checking that it's pointing to something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Raw Data Path is Ok. Found 9003 Files\n",
      "2) Transform File Path is Ok.\n",
      "3) Classifier Config Path is Ok\n",
      "Note: We aren't checking that the files are good, just that the paths exist!\n"
     ]
    }
   ],
   "source": [
    "import os.path, numpy as np, pandas as pd, pickle, random\n",
    "# ^ Multiple imports is a python No-No, but it's fine to save space here...\n",
    "\n",
    "raw_data_path = './hardy/local_data/200504_csv_EIS_simulaiton/'        # 1) Raw Data File Path\n",
    "tform_config_file = './hardy/arbitrage/tform_config.yaml'  # 2) Transformation Config File. (File, not Path!!)\n",
    "classifier_config_path = './hardy/recognition/'            # 3) Classifier COnfiguration Path (Path, not File!!)\n",
    "\n",
    "if os.path.exists(raw_data_path):\n",
    "    print(\"1) Raw Data Path is Ok. Found {} Files\".format(len(os.listdir(raw_data_path))))\n",
    "if os.path.exists(tform_config_file):\n",
    "    print(\"2) Transform File Path is Ok.\")\n",
    "if os.path.exists(classifier_config_path):\n",
    "    print(\"3) Classifier Config Path is Ok\")\n",
    "print(\"Note: We aren't checking that the files are good, just that the paths exist!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Demo On/Off Switch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU ARE WALKING THROUGH THIS NOTEBOOK, TURN THIS ON\n",
    "DEMO = True\n",
    "# IF you just want to run the Classifier, go ahead and turn all the blocks below OFF by making this False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Raw Data *(and File-Structure)*\n",
    "<img src=\"./doc/Images/quickstart_DataImport.PNG\" width=600 p align=\"right\" />\n",
    "\n",
    "First things first, let's talk about your data to make sure we know what we're working with. We are going to read and open up your data with a creative take on the Pandas \"read_csv\" importer, which mostly has the following features:\n",
    " * It allows your data to have a header (by default up to 100 lines) before the actual data. *(It will loop over the rows until the importer finds a \"skiprows\" that imports a good numerical DataFrame)* \n",
    " * We will only keep the columns that contain only Numerical Data, so any word-columns will be simply ignored, as shown.\n",
    " * The Column names will be kept for the final report, but we will refer to them by their Column Number, as shown.\n",
    "\n",
    "### 1b) File Structure and File Names:\n",
    "There are two perfectly good ways we consider to pre-sort your data for the computer to understand it:\n",
    "<img src=\"./doc/Images/quickstart_FileNames.PNG\" width=350 p align=\"right\" />\n",
    "\n",
    " * By using a \"label\" at the end of the file name\n",
    " * By sorting the files into labeled directories beforehand. \n",
    " \n",
    "For now, we will focus on the first method, so please have \"tags\" at the end of your file names: **Defined as any string that comes between the final Underscore and the file extension**. We will hopefully have the other method working soon, but it hasn't been fully tested yet. \n",
    "\n",
    "### 1c) A few other options:\n",
    "In fact, as you'll see later (and we still have some de-bugging to do...), you can either pass a pre-determined list of categories to expect, or use the file name parser to \"figure out\" the categories to use!. \n",
    "\n",
    "By default, we look at the data and expect to find **2 Categories**. If we find more, then the most popular category is category 0, and all of the others are grouped into another category called \"Not_\" + the first category. \n",
    "\n",
    " * In our example dataset, the labels are actually given as \"**noise**\", \"**one**\", and \"**spread**\"\n",
    " * Instead, we report the labels as \"**noise**\" and \"**not_noise**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple File Import:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>freq [Hz]</th>\n",
       "      <th>angular_freq [1/s]</th>\n",
       "      <th>complex_Z [ohm]</th>\n",
       "      <th>Re_Z [ohm]</th>\n",
       "      <th>Im_Z [ohm]</th>\n",
       "      <th>|Z| [ohm]</th>\n",
       "      <th>phase_angle [rad]</th>\n",
       "      <th>Re_Z_noise [ohm]</th>\n",
       "      <th>Im_Z_noise [ohm]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>6.283185e+06</td>\n",
       "      <td>(4.052683096634199e-05-0.006365939721856767j)</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.006366</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>-1.564430</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.006366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>792016.405019</td>\n",
       "      <td>4.976386e+06</td>\n",
       "      <td>(6.460465820414498e-05-0.008037442655613992j)</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.008037</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>-1.562759</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.008037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>627289.985820</td>\n",
       "      <td>3.941379e+06</td>\n",
       "      <td>(0.00010298614656918597-0.010147686456665913j)</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>-0.010148</td>\n",
       "      <td>0.010148</td>\n",
       "      <td>-1.560648</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>-0.010148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>496823.959473</td>\n",
       "      <td>3.121637e+06</td>\n",
       "      <td>(0.0001641662509285629-0.012811686086172225j)</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>-0.012812</td>\n",
       "      <td>0.012813</td>\n",
       "      <td>-1.557983</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>-0.012812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>393492.726310</td>\n",
       "      <td>2.472388e+06</td>\n",
       "      <td>(0.0002616815879154793-0.01617445858945595j)</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>0.016177</td>\n",
       "      <td>-1.554619</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>-0.016174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       freq [Hz]  angular_freq [1/s]  \\\n",
       "0           0  1000000.000000        6.283185e+06   \n",
       "1           1   792016.405019        4.976386e+06   \n",
       "2           2   627289.985820        3.941379e+06   \n",
       "3           3   496823.959473        3.121637e+06   \n",
       "4           4   393492.726310        2.472388e+06   \n",
       "\n",
       "                                  complex_Z [ohm]  Re_Z [ohm]  Im_Z [ohm]  \\\n",
       "0   (4.052683096634199e-05-0.006365939721856767j)    0.000041   -0.006366   \n",
       "1   (6.460465820414498e-05-0.008037442655613992j)    0.000065   -0.008037   \n",
       "2  (0.00010298614656918597-0.010147686456665913j)    0.000103   -0.010148   \n",
       "3   (0.0001641662509285629-0.012811686086172225j)    0.000164   -0.012812   \n",
       "4    (0.0002616815879154793-0.01617445858945595j)    0.000262   -0.016174   \n",
       "\n",
       "   |Z| [ohm]  phase_angle [rad]  Re_Z_noise [ohm]  Im_Z_noise [ohm]  \n",
       "0   0.006366          -1.564430          0.000041         -0.006366  \n",
       "1   0.008038          -1.562759          0.000065         -0.008037  \n",
       "2   0.010148          -1.560648          0.000103         -0.010148  \n",
       "3   0.012813          -1.557983          0.000164         -0.012812  \n",
       "4   0.016177          -1.554619          0.000262         -0.016174  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEMO:\n",
    "    from hardy.handling import handling as handling\n",
    "    files = os.listdir(raw_data_path)\n",
    "    fload = os.path.join(raw_data_path,files[0])\n",
    "    fdata, rows = handling._smart_read_csv(fload, try_skiprows=5)\n",
    "    print(\"Simple File Import:\")\n",
    "else:\n",
    "    fdata = pd.DataFrame()\n",
    "fdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 9003 Files:\n",
      "Found 9000 CSVs...\n",
      "Found 3 Labels, Only Expected 2...\n",
      "\t3500 Files of Label : one\n",
      "\t4500 Files of Label : noise\n",
      "\t1000 Files of Label : spread\n",
      "Loaded\t9000 of 9003\tFiles\t at rate of 304 Files per Second\n",
      "\t Success!\t About 0.49 Minutes...\n",
      "\n",
      "Actual Files Imported. Let's look at the first 'tuple' in the List:\n",
      "Format Will Be :\t(FILE NAME, DataFrame, LABEL)\n",
      "\n",
      "('200504-0001_sim_one',          freq [Hz]  angular_freq [1/s]  Re_Z [ohm]  Im_Z [ohm]  |Z| [ohm]  \\\n",
      "0   1000000.000000        6.283185e+06    0.000041   -0.006366   0.006366   \n",
      "1    792016.405019        4.976386e+06    0.000065   -0.008037   0.008038   \n",
      "2    627289.985820        3.941379e+06    0.000103   -0.010148   0.010148   \n",
      "3    496823.959473        3.121637e+06    0.000164   -0.012812   0.012813   \n",
      "4    393492.726310        2.472388e+06    0.000262   -0.016174   0.016177   \n",
      "..             ...                 ...         ...         ...        ...   \n",
      "75        0.025413        1.596773e-01    1.000000   -0.000004   1.000000   \n",
      "76        0.020128        1.264670e-01    1.000000   -0.000003   1.000000   \n",
      "77        0.015942        1.001640e-01    1.000000   -0.000003   1.000000   \n",
      "78        0.012626        7.933150e-02    1.000000   -0.000002   1.000000   \n",
      "79        0.010000        6.283185e-02    1.000000   -0.000002   1.000000   \n",
      "\n",
      "    phase_angle [rad]  Re_Z_noise [ohm]  Im_Z_noise [ohm]  \n",
      "0           -1.564430          0.000041         -0.006366  \n",
      "1           -1.562759          0.000065         -0.008037  \n",
      "2           -1.560648          0.000103         -0.010148  \n",
      "3           -1.557983          0.000164         -0.012812  \n",
      "4           -1.554619          0.000262         -0.016174  \n",
      "..                ...               ...               ...  \n",
      "75          -0.000004          1.000000         -0.000004  \n",
      "76          -0.000003          1.000000         -0.000003  \n",
      "77          -0.000003          1.000000         -0.000003  \n",
      "78          -0.000002          1.000000         -0.000002  \n",
      "79          -0.000002          1.000000         -0.000002  \n",
      "\n",
      "[80 rows x 8 columns], 'not_noise')\n"
     ]
    }
   ],
   "source": [
    "if DEMO:\n",
    "    # That last file is wrapped into this one to load ALL the files, \n",
    "    # BUT the wrapping function also \n",
    "    from hardy.handling import to_catalogue as catalogue\n",
    "    data_tuples_list = catalogue._data_tuples_from_fnames(raw_data_path)\n",
    "    print(\"\\nActual Files Imported. Let's look at the first 'tuple' in the List:\")\n",
    "    print(\"Format Will Be :\\t(FILE NAME, DataFrame, LABEL)\\n\")\n",
    "    print(data_tuples_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Transformation Configuration Setup\n",
    "\n",
    "The underlying hypothesis of our project is that in addition to using data-rich plots to train a CNN model, that data will almost certainly be even more informative and Data-Rich (thus leading to better model performance) with some number of transformantions such as Log, Reciprocal, and plenty of quadratic transformations as options as well! \n",
    "\n",
    "This is a potentially infinitely expandable part of the project, including that we eventually want to train an algorythm to create 'best guesses' of possible data-rich transformations to try! **For now, though, Users will create their own Configuration File, with the list of transformations to try on their own data.** We have made this quite easy to set up, as shown in this image below:\n",
    "\n",
    "Fundamentally, what you are creating is a simple **Dictionary of Commands**, and then passing (at the top) an **Ordered List** of those commands *(because otherwise python will simply do them in alphabetical order!)* The **Instructions** in the config.yaml file can help you with a bit more explaination than we will do here. Feel free to reach out if further explaination would help.\n",
    "\n",
    "<img src=\"./doc/Images/quickstart_transformconfig.PNG\" width=500 p align=\"right\" />\n",
    "\n",
    "As mentioned, the **Ordered List, called \"tform_command_list\"** will simply be any order of the commands below that you want to run. Also, if you only want to run a few, just leave them off of this list! You don't need to remove them from the dictionary!\n",
    "\n",
    "Finally, the structure of the **\"tform_command_dict\"** is explained a bit more in the image: \n",
    "   * Be sure that the [0] entry is in the range of $0\\Rightarrow5$, since that is like saying \"RGBrgb\" for the X and then Y axis!\n",
    "   * Be sure that the [1] entry is on the **list of Transformations**! That list works a lot like this dictionary in the package, where each keyword will call a function to perform that transformation. As we grow the package, this list will get ever more creative and may include 2D Transformations as well!\n",
    "   * Finally, be sure the [2] entry corresponds to the **Column Numbers** of your raw data (as it will be imported. See section 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded 11 Transforms to Try!\n",
      "Successfully Found and Tested the Transform Config!\n",
      "['Nyquist_like_Z_Z', 'Bode_like_LogF_ZPhase', 'NyBode_like_LogF_ZZ', 'Raw_FZ_Z', 'LogF_RawZZ', 'Recip_FZZ', 'Raw_FZPhase', 'LogF_RawZPhase', 'Recip_FZPhase', 'Raw_FZZ_ZPhase', 'LogF_ZZ_ZPhase']\n",
      "\n",
      "First Transform to run is:\n",
      "\n",
      "Nyquist_like_Z_Z\n",
      "[[0, '1d_raw', 6], [5, '1d_raw', 7]]\n"
     ]
    }
   ],
   "source": [
    "if DEMO: \n",
    "    from hardy.arbitrage import arbitrage as arbitrage\n",
    "    command_list, command_dict = arbitrage.import_tform_config(tform_config_file, raw_df=data_tuples_list[0][1])\n",
    "    print(\"Successfully Found and Tested the Transform Config!\")\n",
    "    print(command_list)\n",
    "    print(\"\\nFirst Transform to run is:\\n\\n{}\".format(command_list[0]))\n",
    "    print(command_dict[command_list[0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Configuring the Model and Tuner\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classifier section of the package, there are two separate configuration files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cnn_config.yaml\n",
    "\n",
    "\n",
    "_A configuration file which contains the hyperparameters to use in the single convolutional neural network.\n",
    "The configuration file is easy to fill out and interact with._ \n",
    "\n",
    "\n",
    "__Note__: Make sure that the hyperparameters found in the config. file are also used in the cnn model\n",
    "\n",
    "\n",
    "<img src=\"./doc/Images/quickstart_cnn_config.PNG\" width=500 p align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tuner_config.yaml\n",
    "    A configuration file containing the hyperparamter search space for the tuning step. This should substitute the single cnn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded 11 Transforms to Try!\n",
      "Processing Data...\tFrom 9002 Files:\n",
      "Found 9000 CSVs...\n",
      "Found 3 Labels, Only Expected 2...\n",
      "\t3500 Files of Label : one\n",
      "\t4500 Files of Label : noise\n",
      "\t1000 Files of Label : spread\n",
      "Loaded\t9000 of 9002\tFiles\t at rate of 201 Files per Second\n",
      "\t Success!\t About 0.75 Minutes...\n",
      "Making rgb Images from Data...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meryp\\Desktop\\DS_Class\\project\\hardy\\hardy\\handling\\visualization.py:65: RuntimeWarning: invalid value encountered in true_divide\n",
      "  normalized_image[:, :, i] = img / (np.amax(img, axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in 24.72seconds!\n",
      "That Took 1.32 Min !\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 237 steps, validate for 27 steps\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Failed to create a directory: .\\../eisy/examples/simulation_data/multi_transform/Nyquist_like_Z_Z/trial_2df434fb6034827fd605ee32096204f1\\checkpoints\\epoch_0; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ca58e4022bcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m run.hardy_multi_transform(raw_data_path, tform_config_path, classifier_config_path,\n\u001b[1;32m----> 2\u001b[1;33m                           iterator_mode='arrays', classes=['noise', ''], project_name='multi_transform')\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\DS_Class\\project\\hardy\\hardy\\run_hardy.py\u001b[0m in \u001b[0;36mhardy_multi_transform\u001b[1;34m(raw_datapath, tform_config_path, classifier_config_path, iterator_mode, plot_format, print_out, num_test_files_class, classifier, split, target_size, batch_size, classes, project_name)\u001b[0m\n\u001b[0;32m    150\u001b[0m                            \u001b[0mimage_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                            \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m                            project_name=project_name)\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;31m# NO OUTPUT? - it outputs the report file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\DS_Class\\project\\hardy\\hardy\\run_hardy.py\u001b[0m in \u001b[0;36mclassifier_wrapper\u001b[1;34m(input_path, test_set_filenames, run_name, config_path, image_data, classifier, iterator_mode, split, target_size, batch_size, image_path, classes, project_name, **kwarg)\u001b[0m\n\u001b[0;32m    293\u001b[0m                                                    run_name)\n\u001b[0;32m    294\u001b[0m         tuned_model = tuner.run_tuner(training_set, validation_set,\n\u001b[1;32m--> 295\u001b[1;33m                                       project_name=output_path)\n\u001b[0m\u001b[0;32m    296\u001b[0m         model, history, metrics = tuner.best_model(tuned_model, training_set,\n\u001b[0;32m    297\u001b[0m                                                    validation_set, test_set)\n",
      "\u001b[1;32m~\\Desktop\\DS_Class\\project\\hardy\\hardy\\recognition\\tuner.py\u001b[0m in \u001b[0;36mrun_tuner\u001b[1;34m(training_set, validation_set, project_name)\u001b[0m\n\u001b[0;32m    168\u001b[0m     tuner.search(training_set, epochs=param['epochs'][0],\n\u001b[0;32m    169\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                  verbose=2, callbacks=[early_stopping])\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtuner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1027\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[0;32m   1121\u001b[0m              'saved.\\n\\nConsider using a TensorFlow optimizer from `tf.train`.')\n\u001b[0;32m   1122\u001b[0m             % (optimizer,))\n\u001b[1;32m-> 1123\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m       \u001b[1;31m# Record this checkpoint so it's visible from tf.train.latest_checkpoint.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m       checkpoint_management.update_checkpoint_state_internal(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, checkpoint_number, session)\u001b[0m\n\u001b[0;32m   1164\u001b[0m       \u001b[0mobject_graph_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1167\u001b[0m     save_path, new_feed_additions = self._save_cached_when_graph_building(\n\u001b[0;32m   1168\u001b[0m         file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m   \"\"\"\n\u001b[1;32m--> 440\u001b[1;33m   \u001b[0mrecursive_create_dir_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m   \"\"\"\n\u001b[1;32m--> 455\u001b[1;33m   \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: .\\../eisy/examples/simulation_data/multi_transform/Nyquist_like_Z_Z/trial_2df434fb6034827fd605ee32096204f1\\checkpoints\\epoch_0; No such file or directory"
     ]
    }
   ],
   "source": [
    "import hardy.run_hardy as run\n",
    "run.hardy_multi_transform(raw_data_path, tform_config_path, classifier_config_path,\n",
    "                          iterator_mode='arrays', classes=['noise', ''], project_name='multi_transform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
