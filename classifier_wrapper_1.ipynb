{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import hardy.recognition.cnn as cnn\n",
    "import hardy.recognition.tuner as tuner\n",
    "from hardy.handling import pre_processing as preprocessing\n",
    "from hardy.handling import handling as handling\n",
    "from hardy.handling import to_catalogue as to_catalogue\n",
    "from hardy.hardy import classifier_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = to_catalogue.save_load_data('../classifier_wrapper/rgb_Rex_Imy', file_extension='.npy', load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../classifier_wrapper/hardy/recognition/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_filenames = preprocessing.hold_out_test_set(path='../200504_csv_EIS_simulation/', number_of_files_per_class=300,\n",
    "                                                     file_extension='.csv', classes=['noise', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./tuner_run_9../classifier_wrapper/test_run_9/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from ./tuner_run_9../classifier_wrapper/test_run_9/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Confusion Matrix \n",
      "\n",
      "[[296   4]\n",
      " [ 39 260]]\n",
      "\n",
      " Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93       300\n",
      "           1       0.98      0.87      0.92       299\n",
      "\n",
      "    accuracy                           0.93       599\n",
      "   macro avg       0.93      0.93      0.93       599\n",
      "weighted avg       0.93      0.93      0.93       599\n",
      "\n",
      "WARNING:tensorflow:From /Users/abdulmoeez/miniconda3/envs/direct/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../200504_csv_EIS_simulation/200605/tuner_run_9/../classifier_wrapper/test_run_9/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "classifier_wrapper('../200504_csv_EIS_simulation/', test_set_filenames, '../classifier_wrapper/test_run_9',\n",
    "                   config_path, image_data=image_data, classifier='tuner',\n",
    "                   iterator_mode='arrays', split= 0.1, target_size= (80,80),\n",
    "                   batch_size=32, image_path=None,\n",
    "                   project_name='tuner_run_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set_filenames = preprocessing.hold_out_test_set(path, number_of_files_per_class=500, classes=[])\n",
    "\n",
    "\n",
    "def classifier_wrapper(input_path, test_set_filenames, run_name, config_path, image_data=None,classifier='tuner',\n",
    "                       iterator_mode='arrays', split= 0.1, target_size= (80,80),\n",
    "                       batch_size=32, image_path=None, classes=['class_1','class_2'],\n",
    "                       project_name='tuner_run', **kwarg):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if iterator_mode == 'arrays':\n",
    "        \n",
    "        assert image_data, 'No image_data list provided'\n",
    "        \n",
    "        test_set_list, learning_set_list = to_catalogue.data_set_split(image_data, test_set_filenames)\n",
    "        \n",
    "        training_set, validation_set = to_catalogue.learning_set(image_list=learning_set_list, split=split, target_size=target_size,\n",
    "                                                                 iterator_mode='arrays', batch_size=batch_size)\n",
    "        \n",
    "        test_set = to_catalogue.test_set(image_list=test_set_list, target_size=target_size,\n",
    "                                         iterator_mode='arrays', batch_size=batch_size)\n",
    "    else:\n",
    "        \n",
    "        assert image_path, 'no path to the image folders was provided'\n",
    "        \n",
    "        training_set, validation_set = to_catalogue.learning_set(image_path, plit=split, target_size=target_size,\n",
    "                                                                 iterator_mode='from_directory', batch_size=batch_size,\n",
    "                                                                 classes=classes)\n",
    "        print(validation_set)\n",
    "        test_set = to_catalogue.test_set(path, target_size=target_size,  classes=classes,\n",
    "                                         iterator_mode='from_directory', batch_size=batch_size,)\n",
    "    \n",
    "    if classifier == 'tuner':\n",
    "#         warn search_function, 'no search function provided, using default RandomSearch'\n",
    "        tuner.build_param(config_path)\n",
    "        tuner_model= tuner.run_tuner(training_set, validation_set, project_name= project_name + run_name)\n",
    "        model, history, metrics = tuner.best_model(tuner_model, training_set, validation_set, test_set)\n",
    "        output_path= preprocessing.save_to_folder(input_path, project_name, run_name)\n",
    "        conf_matrix, report = cnn.report_on_metrics(model, test_set) \n",
    "        tuner.report_generation(model, history, metrics, output_path , tuner=tuner_model, save_model=True)\n",
    "    else:\n",
    "        model, history = cnn.build_model(training_set, validation_set, config_path=config_path)\n",
    "        metrics = cnn.evaluate_model(model, test_set)\n",
    "\n",
    "        output_path= preprocessing.save_to_folder(input_path, project_name, run_name)\n",
    "        conf_matrix, report = cnn.report_on_metrics(model, test_set) \n",
    "        tuner.report_generation(model, history, metrics, output_path , tuner=None, save_model=True, config_path=config_path)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import hardy.run_hardy as run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../eisy/examples/simulation_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tform_config_path = './hardy/arbitrage/tform_config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_config_path = './hardy/recognition/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded 11 Transforms to Try!\n",
      "Processing Data...\tFrom 9002 Files:\n",
      "Found 9000 CSVs...\n",
      "Found 3 Labels, Only Expected 2...\n",
      "\t3500 Files of Label : one\n",
      "\t4500 Files of Label : noise\n",
      "\t1000 Files of Label : spread\n",
      "Loaded\t9000 of 9002\tFiles\t at rate of 57 Files per Second\n",
      "\t Success!\t About 2.63 Minutes...\n",
      "Making rgb Images from Data...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meryp\\Desktop\\DS_Class\\project\\hardy\\hardy\\handling\\visualization.py:65: RuntimeWarning: invalid value encountered in true_divide\n",
      "  normalized_image[:, :, i] = img / (np.amax(img, axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in 98.23seconds!\n",
      "That Took 4.67 Min !\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (8400, 80, 80, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-11819f1dd968>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m run.hardy_multi_transform(raw_data_path, tform_config_path, 'test_run1', classifier_config_path,\n\u001b[1;32m----> 2\u001b[1;33m                           iterator_mode='arrays', classes=['noise', ''], project_name='multi_transform')\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\DS_Class\\project\\hardy\\hardy\\run_hardy.py\u001b[0m in \u001b[0;36mhardy_multi_transform\u001b[1;34m(raw_datapath, tform_config_path, run_name, classifier_config_path, iterator_mode, plot_format, print_out, num_test_files_class, classifier, split, target_size, batch_size, classes, project_name)\u001b[0m\n\u001b[0;32m    111\u001b[0m                            \u001b[0mimage_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                            \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                            project_name=project_name)\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;31m# NO OUTPUT? - it outputs the report file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\DS_Class\\project\\hardy\\hardy\\run_hardy.py\u001b[0m in \u001b[0;36mclassifier_wrapper\u001b[1;34m(input_path, test_set_filenames, run_name, config_path, image_data, classifier, iterator_mode, split, target_size, batch_size, image_path, classes, project_name, **kwarg)\u001b[0m\n\u001b[0;32m    187\u001b[0m         training_set, validation_set = to_catalogue.learning_set(\n\u001b[0;32m    188\u001b[0m             \u001b[0mimage_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_set_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             iterator_mode='arrays', batch_size=batch_size)\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         test_set = to_catalogue.test_set(image_list=test_set_list,\n",
      "\u001b[1;32m~\\Desktop\\DS_Class\\project\\hardy\\hardy\\handling\\to_catalogue.py\u001b[0m in \u001b[0;36mlearning_set\u001b[1;34m(path, split, target_size, classes, batch_size, color_mode, iterator_mode, image_list, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m                                 for i in range(len(image_list))])\n\u001b[0;32m    501\u001b[0m         image_data = image_arrays.reshape(image_arrays.shape[0], n,\n\u001b[1;32m--> 502\u001b[1;33m                                           n, channels).astype('float32')\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[0mimage_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         image_labels = np.array([image_list[i][:][2]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (8400, 80, 80, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "run.hardy_multi_transform(raw_data_path, tform_config_path, 'test_run1', classifier_config_path,\n",
    "                          iterator_mode='arrays', classes=['noise', ''], project_name='multi_transform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_labels = [ 'noisy', 'not_noisy', 'noisy', 'not_noisy' ,'noisy', 'extra']\n",
    "# image_data = [( 'file_1', [1,2,3,4], 'not_noisy'), ( 'file_2', [1,2,3,4], 'noisy'), ( 'file_3', [1,2,3,4], 'not_noisy'), ( 'file_4', [1,2,3,4], 'noisy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['extra', 'noisy', 'not_noisy'], dtype='<U9')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(np.unique(image_labels)):\n",
    "    for j in range(len(image_labels)):\n",
    "        if image_labels[j]== label:\n",
    "            image_labels[j] = i\n",
    "        \n",
    "print(image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
