{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import hardy.recognition.cnn as cnn\n",
    "import hardy.recognition.tuner as tuner\n",
    "from hardy.handling import pre_processing as preprocessing\n",
    "from hardy.handling import handling as handling\n",
    "from hardy.handling import to_catalogue as to_catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = to_catalogue.save_load_data('../eisy/examples/rgb_Rex_Imy', file_extension='.npy', load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './log/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './hardy/recognition/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_filenames = preprocessing.hold_out_test_set(path, number_of_files_per_class=500, classes=[])\n",
    "\n",
    "\n",
    "def classifier_wrapper(input_path, test_set_filenames, run_name, config_path, image_data=None,classifier='tuner',\n",
    "                       iterator_mode='arrays', split= 0.1, target_size= (80,80),\n",
    "                       batch_size=32, image_path=None, classes=['class_1','class_2'],\n",
    "                       project_name='tuner_run', **kwarg):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if iterator_mode == 'arrays':\n",
    "        \n",
    "        assert image_data, 'No image_data list provided'\n",
    "        \n",
    "        test_set_list, learning_set_list = to_catalogue.data_set_split(image_data, test_set_filenames)\n",
    "        \n",
    "        training_set, validation_set = to_catalogue.learning_set(image_list=learning_set_list, split=split, target_size=target_size,\n",
    "                                                                 iterator_mode='arrays', batch_size=batch_size)\n",
    "        \n",
    "        test_set = to_catalogue.test_set(image_list=test_set_list, target_size=target_size,\n",
    "                                         iterator_mode='arrays', batch_size=batch_size)\n",
    "    else:\n",
    "        \n",
    "        assert image_path, 'no path to the image folders was provided'\n",
    "        \n",
    "        training_set, validation_set = to_catalogue.learning_set(image_path, plit=split, target_size=target_size,\n",
    "                                                                 iterator_mode='from_directory', batch_size=batch_size,\n",
    "                                                                 classes=classes)\n",
    "        \n",
    "        test_set = to_catalogue.test_set(path, target_size=target_size,  classes=classes,\n",
    "                                         iterator_mode='from_directory', batch_size=batch_size,)\n",
    "    \n",
    "    if classifier == 'tuner':\n",
    "#         warn search_function, 'no search function provided, using default RandomSearch'\n",
    "        tuner.build_param(config_path)\n",
    "        tuner= tuner.run_tuner(training_set, validation_set, project_name= project_name + transformation_name)\n",
    "        model, history, metrics = tuner.best_model(tuner, training_set, validation_set, test_set)\n",
    "    else:\n",
    "        model, history = cnn.build_model(training_set, validation_set, config_path=config_path)\n",
    "        metrics = cnn.evaluate_model(model, test_set)\n",
    "\n",
    "    output_path= preprocessing.save_to_folder(input_path, project_name, run_name)\n",
    "    conf_matrix, report = cnn.report_on_metrics(model, test_set) \n",
    "    tuner.report_generation(model, history, metrics, output_path , tuner=None, save_model=True, config_path=config_path)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_filenames = preprocessing.hold_out_test_set('../eisy/examples/simulation_data', number_of_files_per_class=150, classes=['noise',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(np.unique(test_set_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_list, learning_set_list = to_catalogue.data_set_split(image_data, test_set_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8700"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learning_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out_test_set(path=None, number_of_files_per_class=100, classes=['noise', ''],\n",
    "                      file_extension='.csv', image_list=None, iterator_mode=None):\n",
    "    '''\n",
    "    Functions that returns a list of filenames\n",
    "    of the randomly selected files to compose the test set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "           string containing the path to the files to select from\n",
    "           the test set from.\n",
    "\n",
    "    number_of_files_per_class: int\n",
    "                               The number of files to select from each class.\n",
    "\n",
    "    classes: list\n",
    "             a list containing strings of the classes the data is divided in.\n",
    "             The classes are contained in the filename as labels.\n",
    "\n",
    "    file_extension: str\n",
    "                    the extension of the file to read. The default value is\n",
    "                    .csv\n",
    "    image_list: np.array\n",
    "                numpy array representing file names, image data and labels\n",
    "    iterator_mode: str\n",
    "                   string representing if the data provided is in arrays\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_set_serialnumbers : list\n",
    "                             A list containig the strings of filenames\n",
    "                             randomly selected to be part of the test set.\n",
    "    '''\n",
    "\n",
    "    # Initialize a list that will contain the serial numbers of thefiles\n",
    "    # composing the test set\n",
    "    test_set_filenames = []\n",
    "    \n",
    "    # seperating test_set_filenames for input as arrays\n",
    "    if iterator_mode == \"arrays\":\n",
    "        file_list_1 = [n[0] for n in image_data if n[0].endswith(classes[0])]\n",
    "        file_list_2 = [n[0] for n in image_data if not n[0].endswith(classes[0])]\n",
    "        for i in range(number_of_files_per_class):\n",
    "            chosen_file = random.choice(file_list_1)\n",
    "            file_list_1.remove(chosen_file)\n",
    "            test_set_filenames.append(str(chosen_file))\n",
    "            chosen_file = random.choice(file_list_2)\n",
    "            file_list_2.remove(chosen_file)\n",
    "            test_set_filenames.append(str(chosen_file))\n",
    "    # # These lines are hardcoded to allow for 2 classes only # #\n",
    "    # #  Rewrite to support a higher number of classes # #\n",
    "\n",
    "    # Randomly pick files that are labelled as noisy and append\n",
    "    #  them into the test_set list\n",
    "    else:\n",
    "        file_list_1 = [n for n in os.listdir(path)\n",
    "                    if n.endswith(classes[0]+file_extension)]\n",
    "        file_list_2 = [n for n in os.listdir(path)\n",
    "                    if not n.endswith(classes[0]+file_extension)]\n",
    "        for i in range(number_of_files_per_class):\n",
    "            chosen_file = random.choice(file_list_1)\n",
    "            file_list_1.remove(chosen_file)\n",
    "            test_set_filenames.append(str(chosen_file.rstrip(chosen_file[-4:])\n",
    "                                        ))\n",
    "\n",
    "            chosen_file = random.choice(file_list_2)\n",
    "            file_list_2.remove(chosen_file)\n",
    "            test_set_filenames.append(str(chosen_file.rstrip(chosen_file[-4:])\n",
    "                                        ))\n",
    "\n",
    "    return test_set_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuner_from_param(self, config_path):\n",
    "\n",
    "    param = cnn.import_config(config_path)\n",
    "\n",
    "    def build_tuner_model(hp):\n",
    "        '''\n",
    "        Functions that builds a convolutional keras model with\n",
    "        tunable hyperparameters\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hp: keras tuner class\n",
    "            A class that is used to define the parameter search space\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model: Keras sequential model\n",
    "               The trained convolutional neural network\n",
    "        '''\n",
    "        ###################################\n",
    "        # loading the configuration file for tuner\n",
    "\n",
    "        ####################################\n",
    "        # Defining input size\n",
    "\n",
    "        inputs = tf.keras.Input(shape=(50, 50, 3))\n",
    "        x = inputs\n",
    "\n",
    "        ####################################\n",
    "        # extracting parameters from the parameters file\n",
    "        # and feeding in the tuner\n",
    "\n",
    "        for i in range(hp.Int('conv_layers', 1, max(param['layers']),\n",
    "                              default=3)):\n",
    "            x = tf.keras.layers.Conv2D(\n",
    "                filters=getattr(hp, param['filters'][0])\n",
    "                ('filters_' + str(i), min(param['filters'][1]['values']),\n",
    "                 max(param['filters'][1]['values']), step=4, default=8),\n",
    "                kernel_size=getattr(hp, param['kernel_size'][0])\n",
    "                ('kernel_size_' + str(i), min(param['kernel_size'\n",
    "                                                    ][1]['values']),\n",
    "                 max(param['kernel_size'][1]['values'])),\n",
    "                activation=getattr(hp, param['activation'][0])\n",
    "                ('activation_' + str(i), values=param['activation'\n",
    "                                                      ][1]['values']),\n",
    "                padding='same')(x)\n",
    "\n",
    "        if getattr(hp,\n",
    "                   param['pooling'][0])('pooling',\n",
    "                                        values=param['pooling'][1]['values'])\\\n",
    "                == 'max':\n",
    "            x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "        else:\n",
    "            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        outputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "        # adding in the optimizer\n",
    "        optimizer = getattr(hp, param['optimizer'][0]\n",
    "                            )('optimizer', values=param['optimizer'\n",
    "                                                        ][1]['values'])\n",
    "\n",
    "        # compiling neural network model\n",
    "        model.compile(optimizer, loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
